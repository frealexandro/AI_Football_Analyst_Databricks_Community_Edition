{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2724a44-bf21-4f5d-8886-4617b47c11d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ⚽ AI Football Analyst - Setup & Configuration\n",
    "\n",
    "**Fase 0: Setup Inicial - Community Edition Compatible**\n",
    "- Instalar librerías necesarias\n",
    "- Configurar rutas del workspace\n",
    "- Validar Delta Lake\n",
    "- Crear funciones de utilidad\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b77927d-5f99-495a-9d1e-40d5a1f75940",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Instalación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfc34b84-ddb7-49f0-81df-168f20183a54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in /databricks/python3/lib/python3.11/site-packages (5.9.0)\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.11/site-packages (1.3.0)\nRequirement already satisfied: xgboost in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2c8431a7-e7ac-4952-8278-0c04e9881af6/lib/python3.11/site-packages (3.1.1)\nRequirement already satisfied: tenacity>=6.2.0 in /databricks/python3/lib/python3.11/site-packages (from plotly) (8.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn) (1.23.5)\nRequirement already satisfied: scipy>=1.5.0 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn) (1.11.1)\nRequirement already satisfied: joblib>=1.1.1 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Instalar librerías (sin MLflow explícito, ya viene con Databricks)\n",
    "%pip install plotly scikit-learn xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40c2db70-4da8-4e77-b465-97a182495864",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Configuración de Rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14a725c0-a1be-4db2-a777-d9d787b178a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDC64 Usuario: randryan308@gmail.com\n\n============================================================\n\uD83D\uDCC2 RUTAS DEL PROYECTO\n============================================================\nBase: /Workspace/Users/randryan308@gmail.com/football_analyst\n============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "# Usuario actual\n",
    "user_email = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "print(f\"\uD83D\uDC64 Usuario: {user_email}\")\n",
    "\n",
    "# Rutas base\n",
    "BASE_PATH = f\"/Workspace/Users/{user_email}/football_analyst\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\uD83D\uDCC2 RUTAS DEL PROYECTO\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Base: {BASE_PATH}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f93476d-d8df-4ee5-89ac-e74735dfad93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Delta Tables Planificadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05372d1a-5887-409b-96ba-3001ede2412e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n\uD83D\uDCCA DELTA TABLES PLANIFICADAS\n============================================================\n\nBRONZE:\n  - football_players_raw\n  - football_teams_raw\n  - football_matches_raw\n\nSILVER:\n  - football_players_features\n  - football_teams_features\n  - football_matches_features\n\nGOLD:\n  - football_predictions\n============================================================\n"
     ]
    }
   ],
   "source": [
    "DELTA_TABLES = {\n",
    "    \"bronze\": [\"football_players_raw\", \"football_teams_raw\", \"football_matches_raw\"],\n",
    "    \"silver\": [\"football_players_features\", \"football_teams_features\", \"football_matches_features\"],\n",
    "    \"gold\": [\"football_predictions\"]\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"\uD83D\uDCCA DELTA TABLES PLANIFICADAS\")\n",
    "print(\"=\" * 60)\n",
    "for layer, tables in DELTA_TABLES.items():\n",
    "    print(f\"\\n{layer.upper()}:\")\n",
    "    for table in tables:\n",
    "        print(f\"  - {table}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d719da10-a671-4a59-8ea9-80b8ab558ac8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Funciones de Utilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "137704de-d44b-4184-83e1-be044a81f197",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Funciones listas\n"
     ]
    }
   ],
   "source": [
    "def save_to_delta(df: DataFrame, table_name: str, mode: str = \"overwrite\"):\n",
    "    \"\"\"Guarda DataFrame en Delta Table\"\"\"\n",
    "    print(f\"\uD83D\uDCBE Guardando: {table_name}\")\n",
    "    df.write.format(\"delta\").mode(mode).saveAsTable(table_name)\n",
    "    count = spark.table(table_name).count()\n",
    "    print(f\"✅ {count:,} registros\")\n",
    "\n",
    "\n",
    "def load_from_delta(table_name: str) -> DataFrame:\n",
    "    \"\"\"Carga DataFrame desde Delta Table\"\"\"\n",
    "    df = spark.table(table_name)\n",
    "    print(f\"\uD83D\uDCD6 {table_name}: {df.count():,} registros\")\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"✅ Funciones listas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cc13c62-9ef2-4a5b-a79f-7692d24375b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Validar Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c44dc2e-3b08-4419-8f28-4588dd5a7271",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD0D Validando Delta Lake...\n✅ Delta Lake OK (1 registro)\n"
     ]
    }
   ],
   "source": [
    "print(\"\uD83D\uDD0D Validando Delta Lake...\")\n",
    "\n",
    "# Test\n",
    "test_df = spark.createDataFrame([(1, \"Test\")], [\"id\", \"name\"])\n",
    "test_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"test_table\")\n",
    "result = spark.table(\"test_table\").count()\n",
    "spark.sql(\"DROP TABLE IF EXISTS test_table\")\n",
    "\n",
    "print(f\"✅ Delta Lake OK ({result} registro)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4674daf-573c-4291-8e30-f057f66fef70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Verificar MLflow (opcional en Community Edition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12d4d156-106a-4717-a961-8e930e6bdcf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/mlflow/protos/service_pb2.py:11: UserWarning: google.protobuf.service module is deprecated. RPC implementations should provide code generator plugins which generate code specific to the RPC implementation. service.py will be removed in Jan 2025\n  from google.protobuf import service as _service\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD0D Verificando MLflow...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 19:42:59,485 25906 ERROR _handle_rpc_error GRPC Error received\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py\", line 1862, in config\n    resp = self._stub.Config(req, metadata=self.metadata())\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py\", line 277, in __call__\n    response, ignored_call = self._with_call(\n                             ^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py\", line 332, in _with_call\n    return call.result(), call\n           ^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/grpc/_channel.py\", line 440, in result\n    raise self\n  File \"/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py\", line 315, in continuation\n    response, call = self._thunk(new_method).with_call(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/grpc/_channel.py\", line 1198, in with_call\n    return _end_unary_response_blocking(state, call, True, None)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/grpc/_channel.py\", line 1006, in _end_unary_response_blocking\n    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ngrpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {created_time:\"2025-11-13T19:42:59.484351451+00:00\", grpc_status:13, grpc_message:\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\"}\"\n>\n2025-11-13 19:42:59,485 25906 ERROR _handle_rpc_error GRPC Error received\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py\", line 1862, in config\n    resp = self._stub.Config(req, metadata=self.metadata())\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py\", line 277, in __call__\n    response, ignored_call = self._with_call(\n                             ^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py\", line 332, in _with_call\n    return call.result(), call\n           ^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/grpc/_channel.py\", line 440, in result\n    raise self\n  File \"/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py\", line 315, in continuation\n    response, call = self._thunk(new_method).with_call(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/grpc/_channel.py\", line 1198, in with_call\n    return _end_unary_response_blocking(state, call, True, None)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/grpc/_channel.py\", line 1006, in _end_unary_response_blocking\n    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ngrpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {created_time:\"2025-11-13T19:42:59.484351451+00:00\", grpc_status:13, grpc_message:\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\"}\"\n>\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ MLflow limitado en Community Edition\n   (No es crítico, podemos continuar)\n   Error: [CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "print(\"\uD83D\uDD0D Verificando MLflow...\")\n",
    "\n",
    "try:\n",
    "    # Intentar configurar experiment (puede fallar en Community Edition)\n",
    "    experiment_name = f\"/Users/{user_email}/football_analyst\"\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    print(f\"✅ MLflow configurado: {experiment_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ MLflow limitado en Community Edition\")\n",
    "    print(f\"   (No es crítico, podemos continuar)\")\n",
    "    print(f\"   Error: {str(e)[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e85415e4-8e32-459f-a2d1-7d788d2e75ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "436ba7d5-5b52-4184-83fd-04dd7f57dd99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n\uD83C\uDF89 SETUP COMPLETADO\n================================================================================\n\n\uD83D\uDC64 Usuario: randryan308@gmail.com\n\uD83D\uDCC2 Workspace: /Workspace/Users/randryan308@gmail.com/football_analyst\n\uD83D\uDCCA Delta Tables: 7 planificadas\n\n✅ Stack Databricks:\n   - Delta Lake ✅\n   - Spark ✅\n   - MLflow (limitado en Community Edition)\n\n\uD83D\uDE80 Listo para FASE 1\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"\uD83C\uDF89 SETUP COMPLETADO\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n\uD83D\uDC64 Usuario: {user_email}\")\n",
    "print(f\"\uD83D\uDCC2 Workspace: {BASE_PATH}\")\n",
    "print(f\"\uD83D\uDCCA Delta Tables: {sum(len(v) for v in DELTA_TABLES.values())} planificadas\")\n",
    "print(f\"\\n✅ Stack Databricks:\")\n",
    "print(\"   - Delta Lake ✅\")\n",
    "print(\"   - Spark ✅\")\n",
    "print(\"   - MLflow (limitado en Community Edition)\")\n",
    "print(f\"\\n\uD83D\uDE80 Listo para FASE 1\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68cb51ca-cb2b-45a3-9a83-293564eed63f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Próximos Pasos\n",
    "\n",
    "**FASE 1:** Ejecutar `notebooks/01_data_ingestion/simple_download`\n",
    "- Descarga dataset EPL con curl\n",
    "- Carga datos a Delta Tables"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_setup_config",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}